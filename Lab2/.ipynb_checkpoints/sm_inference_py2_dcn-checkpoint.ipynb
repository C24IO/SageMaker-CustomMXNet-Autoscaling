{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Let's walk through deploying a DCN model in Sagemaker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import io\n",
    "from matplotlib.pyplot import imshow, imread\n",
    "import subprocess\n",
    "from skimage.transform import resize as skresize\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.predictor import StringDeserializer\n",
    "from sagemaker.predictor import RealTimePredictor, json_deserializer\n",
    "import image_pb2 as impb\n",
    "import ast\n",
    "from IPython.display import clear_output\n",
    "from dcn.lib.utils.show_boxes import show_boxes\n",
    "\n",
    "\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "bucket = 'ar54' #sess.default_bucket() # can replace with our own bucket \n",
    "role = sagemaker.session.get_execution_role()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#!cp /home/ec2-user/SageMaker/SageMaker-Inference-Advanced/n-labs/11-manual-model-load/dcn/model/rfcn_dcn_coco-0000.params dcn/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at our inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deploy our model to an endpoint, we need to put our weights into gzip format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd dcn/model/ && tar -czvf /tmp/model-rfcn.tar.gz *)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('/tmp/model-rfcn.tar.gz', bucket, 'super_models/model-rfcn.tar.gz')\n",
    "!aws s3 ls s3://ar54/super_models/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our container we will use for DCN inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=mxnet-serving-160-gpu-py2\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "#region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "#docker build -t chazarey-mxnet-serving:1.6.0-gpu-py3 -f docker/1.6.0/py3/Dockerfile.gpu .\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.gpu .\n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built our container, we can deploy our endpoint. This process usually takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "\n",
    "model_data=\"s3://ar54/super_models/model-rfcn.tar.gz\"\n",
    "\n",
    "model = MXNetModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    image=\"308412838853.dkr.ecr.us-east-2.amazonaws.com/mxnet-serving-160-gpu-py2:latest\",\n",
    "    entry_point=\"inference.py\",\n",
    "    py_version='py2',\n",
    "    framework_version='1.6.0',\n",
    "    enable_cloudwatch_metrics=True\n",
    ")\n",
    "\n",
    "# for deploying the endpoint locally for testing we can use an instance_type of local\n",
    "#predictor = model.deploy(instance_type=\"local_gpu\", initial_instance_count=1)\n",
    "#predictor = model.deploy(instance_type=\"local\", initial_instance_count=1)\n",
    "\n",
    "predictor = model.deploy(instance_type='ml.g4dn.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 81\n",
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "           'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "           'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "           'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "           'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "           'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "           'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "           'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
    "           'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have deployed our endpoint, let's test it out!\n",
    "We are going to package our image into a protobuf object and then send it to our endpoint for classification. \n",
    "Right now it takes approximately 60ms end to end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = '/home/ec2-user/SageMaker/GitHub/SageMaker-CustomMXNet-Autoscaling/Lab2/dcn/demo/COCO_test2015_000000000891.jpg'\n",
    "\n",
    "img = imread(impath)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = '/home/ec2-user/SageMaker/GitHub/SageMaker-CustomMXNet-Autoscaling/Lab2/dcn/demo/769452-1.2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(impath, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "image_packet = impb.PBImage()\n",
    "image_packet.image_data = payload\n",
    "\n",
    "predictor.serializer = None\n",
    "predictor.deserializer = StringDeserializer()\n",
    "predictor.accept = None\n",
    "predictor.content_type = 'application/octet-stream'\n",
    "\n",
    "response = predictor.predict(image_packet.SerializeToString())          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/ec2-user/SageMaker/GitHub/SageMaker-CustomMXNet-Autoscaling/Lab2/dcn/demo/769452-1.2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our image prior to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = '/home/ec2-user/SageMaker/GitHub/SageMaker-CustomMXNet-Autoscaling/Lab2/dcn/demo/769452-1.2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resp_list = []\n",
    "for e in eval(response)['array']:\n",
    "    resp_list.append(np.array(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im = cv2.imread('./dcn/demo/' + im_name)\n",
    "im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "show_boxes(im, resp_list, classes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = '/home/ec2-user/SageMaker/GitHub/SageMaker-CustomMXNet-Autoscaling/Lab2/dcn/demo/769452-1.2.jpg'\n",
    "#impath = '/home/ec2-user/SageMaker/GitHub/SageMaker-CustomMXNet-Autoscaling/Lab2/dcn/demo/769452-2.4.jpg'\n",
    "img = imread(impath)\n",
    "\n",
    "with open(impath, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "image_packet = impb.PBImage()\n",
    "image_packet.image_data = payload\n",
    "\n",
    "predictor.serializer = None\n",
    "predictor.deserializer = StringDeserializer()\n",
    "predictor.accept = None\n",
    "predictor.content_type = 'application/octet-stream'\n",
    "\n",
    "%timeit response = predictor.predict(image_packet.SerializeToString())          \n",
    "\n",
    "resp_list = []\n",
    "for e in eval(response)['array']:\n",
    "    resp_list.append(np.array(e))\n",
    "    \n",
    "im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "show_boxes(im, resp_list, classes, 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "endpoint_name=predictor.endpoint\n",
    "total_runs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running {} inferences for {}:'.format(total_runs, endpoint_name))\n",
    "\n",
    "client_times = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "for i in range(total_runs):    \n",
    "    \n",
    "    client_start = time.time()\n",
    "    \n",
    "    response = predictor.predict(image_packet.SerializeToString())\n",
    "        \n",
    "    client_end = time.time()\n",
    "    client_times.append((client_end - client_start)*1000)\n",
    "\n",
    "\n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "period = int(period)\n",
    "\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "  # Should be 1000\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / total_runs\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / total_runs\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / total_runs\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / total_runs\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / total_runs\n",
    "    print('Avg | P50 | P90 | P95 | P100')\n",
    "    print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p100))\n",
    "\n",
    "    cloudwatch_ready = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(80, 60))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(client_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
